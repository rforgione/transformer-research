{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c41f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbd1190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 08:39:49.990757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15a8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d57d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2168e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 08:39:53.334886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-01 08:39:53.352834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-01 08:39:53.353020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-01 08:39:54.409287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-01 08:39:54.409627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-01 08:39:54.409644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-01 08:39:54.409890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-01 08:39:54.409931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 7335 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0904d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0f77a",
   "metadata": {},
   "source": [
    "First step is to create an embedding layer that translates raw tokens into K-dimensional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eacfc38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.0491559   0.04126722 -0.04697653 ... -0.0068565  -0.03183677\n",
      "   -0.03195263]\n",
      "  [ 0.01929628 -0.01089764  0.01000128 ... -0.03021339 -0.01418394\n",
      "   -0.00332562]\n",
      "  [-0.01274476  0.04068748  0.04470846 ... -0.04423631  0.04037298\n",
      "    0.04162011]\n",
      "  ...\n",
      "  [-0.02374609  0.03028352 -0.032427   ...  0.0421473   0.04265909\n",
      "   -0.04501848]\n",
      "  [-0.00558231  0.00323743 -0.03707803 ... -0.02362527 -0.04564954\n",
      "    0.0056197 ]\n",
      "  [-0.03696796 -0.02114703 -0.03012    ... -0.03253096 -0.00911947\n",
      "    0.00583986]]], shape=(1, 10, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Your input text\n",
    "input_text = \"John is the best basketball player I know\"\n",
    "\n",
    "# Tokenize the text\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Extract input_ids\n",
    "input_ids = input_tokens[\"input_ids\"]\n",
    "\n",
    "# Create an embedding layer\n",
    "vocab_size = tokenizer.vocab_size  # Size of the BERT vocabulary\n",
    "embedding_dim = 768  # Embedding dimension, which is 768 for BERT-base\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    trainable=True,\n",
    ")\n",
    "\n",
    "# Convert token IDs to embeddings\n",
    "token_embeddings = embedding_layer(input_ids)\n",
    "\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e0d1e",
   "metadata": {},
   "source": [
    "# Positional Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad6d48",
   "metadata": {},
   "source": [
    "In the paper \"Attention is All You Need\", they define the positional encodings as follows:\n",
    "\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "\n",
    "The first thing that we want to do is break down the denominator which is the same in both cases.\n",
    "\n",
    "```\n",
    "10000^(2i/d_model) = exp(2i * (1/d_model) * log(10000)\n",
    "```\n",
    "\n",
    "Thus we can remove the exponent, which should promote numerical stability throughout the computations. \n",
    "\n",
    "We know that `i` represents the position within the embedding, so the first thing that we can do is create a range of length `d_model` with the function `f(i) = 2i`. We do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97c5140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d30a4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.arange(max_position)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1acfe381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef99f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d6d043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_i = np.arange(0, d_model, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "678280f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,\n",
       "        26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "        52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,\n",
       "        78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102,\n",
       "       104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128,\n",
       "       130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154,\n",
       "       156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180,\n",
       "       182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206,\n",
       "       208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232,\n",
       "       234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258,\n",
       "       260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284,\n",
       "       286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310,\n",
       "       312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336,\n",
       "       338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362,\n",
       "       364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,\n",
       "       390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414,\n",
       "       416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440,\n",
       "       442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466,\n",
       "       468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492,\n",
       "       494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518,\n",
       "       520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544,\n",
       "       546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570,\n",
       "       572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596,\n",
       "       598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622,\n",
       "       624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648,\n",
       "       650, 652, 654, 656, 658, 660, 662, 664, 666, 668, 670, 672, 674,\n",
       "       676, 678, 680, 682, 684, 686, 688, 690, 692, 694, 696, 698, 700,\n",
       "       702, 704, 706, 708, 710, 712, 714, 716, 718, 720, 722, 724, 726,\n",
       "       728, 730, 732, 734, 736, 738, 740, 742, 744, 746, 748, 750, 752,\n",
       "       754, 756, 758, 760, 762, 764, 766])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a103a3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(two_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd0488",
   "metadata": {},
   "source": [
    "We only need the divisors to be of length 384 because the the divisors are the same for the evens and the odd numbers. We can create the array once and then use it for both the even and the odd assignment, which we'll move to next.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d05e13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = np.exp(two_i * 1/d_model * np.log(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709c321",
   "metadata": {},
   "source": [
    "Now we can go ahead and build the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f8f30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 10\n",
    "d_model = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71036b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_matrix = np.zeros((num_tokens, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5af58617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d56241",
   "metadata": {},
   "source": [
    "Looking good so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2075bda",
   "metadata": {},
   "source": [
    "You can see now why we only needed `d_model / 2` elements in our array `two_i` -- we do two sets of assignments, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38ad86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_matrix[:, 0::2] = np.sin(position / denominator)\n",
    "pos_matrix[:, 1::2] = np.cos(position / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee3719cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.82843076,  0.56009149,  0.81525065,\n",
       "         0.57910826,  0.8019618 ,  0.59737533,  0.78859304,  0.61491545],\n",
       "       [ 0.90929743, -0.41614684,  0.92799403, -0.37259506,  0.94423677,\n",
       "        -0.32926724,  0.95814438, -0.28628544,  0.9698361 , -0.24375797],\n",
       "       [ 0.14112001, -0.9899925 ,  0.21109235, -0.97746612,  0.27837998,\n",
       "        -0.96047102,  0.34278182, -0.93941504,  0.40414137, -0.91469654],\n",
       "       [-0.7568025 , -0.65364362, -0.69153198, -0.72234585, -0.62181248,\n",
       "        -0.78316616, -0.54860557, -0.83608129, -0.47281055, -0.88116411],\n",
       "       [-0.95892427,  0.28366219, -0.98573469,  0.1683066 , -0.99857347,\n",
       "         0.05339503, -0.99822869, -0.05949362, -0.9856184 , -0.16898632],\n",
       "       [-0.2794155 ,  0.96017029, -0.41267124,  0.91088004, -0.53475181,\n",
       "         0.84500917, -0.6440288 ,  0.76500125, -0.73933342,  0.67333951],\n",
       "       [ 0.6569866 ,  0.75390225,  0.5234674 ,  0.8520457 ,  0.37921509,\n",
       "         0.92530855,  0.22877486,  0.97347936,  0.07636331,  0.99708006],\n",
       "       [ 0.98935825, -0.14550003,  0.99905051,  0.04356705,  0.97396499,\n",
       "         0.22669848,  0.91735771,  0.39806385,  0.83324738,  0.55290036],\n",
       "       [ 0.41211849, -0.91113026,  0.59565196, -0.80324264,  0.74884726,\n",
       "        -0.66274263,  0.86723886, -0.49789231,  0.94839007, -0.3171061 ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix[:10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcdcd6",
   "metadata": {},
   "source": [
    "Notice how the broadcasting works -- we used a matrix of shape (num_tokens, 1) to represent the different token positions, and we used vectors of shape (1, d_model) -- or really, two vectors of size (1, d_model / 2) -- to repreesnt the latent dimensions. The result is an output with shape (num_tokens, d_model), which is our original matrix shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2aff2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_matrix_tf = tf.constant(pos_matrix, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21791803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 768), dtype=float32, numpy=\n",
       "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "       [ 8.4147096e-01,  5.4030228e-01,  8.2843077e-01, ...,\n",
       "         1.0000000e+00,  1.0242752e-04,  1.0000000e+00],\n",
       "       [ 9.0929741e-01, -4.1614684e-01,  9.2799401e-01, ...,\n",
       "         1.0000000e+00,  2.0485504e-04,  1.0000000e+00],\n",
       "       ...,\n",
       "       [ 6.5698659e-01,  7.5390226e-01,  5.2346742e-01, ...,\n",
       "         9.9999970e-01,  7.1699260e-04,  9.9999976e-01],\n",
       "       [ 9.8935825e-01, -1.4550003e-01,  9.9905050e-01, ...,\n",
       "         9.9999964e-01,  8.1942009e-04,  9.9999964e-01],\n",
       "       [ 4.1211849e-01, -9.1113025e-01,  5.9565198e-01, ...,\n",
       "         9.9999958e-01,  9.2184759e-04,  9.9999958e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3765f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings_with_pos_encoding = token_embeddings + pos_matrix_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
